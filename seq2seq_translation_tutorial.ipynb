{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
    "*******************************************************************************\n",
    "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
    "\n",
    "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
    "write our own classes and functions to preprocess the data to do our NLP\n",
    "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
    "learn how `torchtext` can handle much of this preprocessing for you in the\n",
    "three tutorials immediately following this one.\n",
    "\n",
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "\n",
    "::\n",
    "\n",
    "    [KEY: > input, = target, < output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "... to varying degrees of success.\n",
    "\n",
    "This is made possible by the simple but powerful idea of the `sequence\n",
    "to sequence network <https://arxiv.org/abs/1409.3215>`__, in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "To improve upon this model we'll use an `attention\n",
    "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-  https://pytorch.org/ For installation instructions\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
    "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
    "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
    "\n",
    "\n",
    "It would also be useful to know about Sequence to Sequence networks and\n",
    "how they work:\n",
    "\n",
    "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation <https://arxiv.org/abs/1406.1078>`__\n",
    "-  `Sequence to Sequence Learning with Neural\n",
    "   Networks <https://arxiv.org/abs/1409.3215>`__\n",
    "-  `Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
    "-  `A Neural Conversational Model <https://arxiv.org/abs/1506.05869>`__\n",
    "\n",
    "You will also find the previous tutorials on\n",
    ":doc:`/intermediate/char_rnn_classification_tutorial`\n",
    "and :doc:`/intermediate/char_rnn_generation_tutorial`\n",
    "helpful as those concepts are very similar to the Encoder and Decoder\n",
    "models, respectively.\n",
    "\n",
    "And for more, read the papers that introduced these topics:\n",
    "\n",
    "-  `Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "   Statistical Machine Translation <https://arxiv.org/abs/1406.1078>`__\n",
    "-  `Sequence to Sequence Learning with Neural\n",
    "   Networks <https://arxiv.org/abs/1409.3215>`__\n",
    "-  `Neural Machine Translation by Jointly Learning to Align and\n",
    "   Translate <https://arxiv.org/abs/1409.0473>`__\n",
    "-  `A Neural Conversational Model <https://arxiv.org/abs/1506.05869>`__\n",
    "\n",
    "\n",
    "**Requirements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "==================\n",
    "\n",
    "The data for this project is a set of many thousands of English to\n",
    "French translation pairs.\n",
    "\n",
    "`This question on Open Data Stack\n",
    "Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
    "pointed me to the open translation site https://tatoeba.org/ which has\n",
    "downloads available at https://tatoeba.org/eng/downloads - and better\n",
    "yet, someone did the extra work of splitting language pairs into\n",
    "individual text files here: https://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repo, so\n",
    "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
    "separated list of translation pairs:\n",
    "\n",
    "::\n",
    "\n",
    "    I am cold.    J'ai froid.\n",
    "\n",
    ".. Note::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the ``reverse``\n",
    "flag to reverse the pairs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('Khasi_English.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "    print(lines)    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split(',')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['Khublei!,Thank you!', 'Balei?,Why?', 'Pynjaijai,Cool off!', 'Rieh!,Take cover!', 'Peit,Watch', 'Mih!,Leave!', 'Phet!,Scram!', 'Iehnoh!,Leave!', 'Tan!,Pull!', 'Pyneh!,Pull yourself together!', 'Adi!,Ouch!', 'Ada!,Ouch!', 'Bam!,Eat!', 'Ieng!,Stand up!', 'Shuh,Stand aside', 'Tam,Pick it up', 'Khar,Pick it up', 'Hooid,Yes', 'Haoid,Yes', 'Em,No', 'Nai,No', \"Map,I'm sorry\", 'Kloi,Hurry up', 'Wut,Hurry up', 'Lait,Wrong', 'Kaei?,What?', 'Uei?,What?', 'Iei?,What?', 'Ani!,Wow!', 'Iap?,Dead?', 'Hatshi!,Achoo!', 'Shisha!,Really?', 'Kumno!,Hello!', 'Ei!,Hello!', 'Biang!,Perfect!', 'Sngewbha,Please', 'Bew!,Fuck!', 'Biw!,Fuck!', 'Bow!,Fuck!', 'Iarap!,Help!', 'Sangeh!,Stop!', 'Thngan?,Hungry?', 'Bieij,Crazy!', 'Eit,Shit', 'Bleh,Blah', 'Thik!,Precisely!', \"Bymlahlong!,That's impossible\", 'Kamkai!,Nonsense!', 'Ïap!,Die!', 'Ho!,Goodbye!', 'Mo!,Goodbye!', 'Ei,Hi', 'Kadei!,Correct!', 'Dei!,Correct!', 'Kynthih,Jump', 'Ryngkoh,Jump', 'Shhhh!,Hush!', 'Bieij!,Idiot!', 'Kale!,Idiot!', 'Sniang!,Idiot!', 'Te,Duh', 'Sdieh,Fry', 'Kynmaw!,Remember!', 'Wan!,Come!', 'Mareh!,Run!', 'Bymlahlong,Impossible', 'Ihiw!,Hooray!', 'Kamkai,Nonsense', 'Phylla,Strange', 'Te?,So?', 'Rung!,Enter!', 'Shuh!,Scram!', 'Jarjar!,Quiet!', 'Sngapjar!,Quiet!', 'Pyrshang!,Try!', 'Sdang!,Begin!', 'Kren!,Speak!', 'Peit!,Look!', 'Kloi!,Quick!', 'Wut!,Quick!', 'Siat!,Fire!', 'Kumno,Hello', 'Sngap,Listen', 'Mano?,Who?', 'Jubab!,Answer!', 'Jaituh,Lazybones', 'Thoh!,Write!', 'Shabar!,Out!', 'Leit,Go', 'Khie,Go', 'Dem!,Kneel!', 'Nguh!,Kneel!', 'To,Alright', 'Ap,Wait', 'Shibun!,Very!', 'Aw!,Ow!', 'Em?,Right?', 'Kumjuh,Likewise', 'Nga!,Me!', 'Manga!,Me!', 'Hangne?,Here?', 'Shane?,Here?', 'Jied,Choose', 'Sngewthuh?,Understood?', 'Jaijai,Chill', 'Her!,Fly!', 'Iaid,Walk', 'Husiar!,Beware!', 'Bang!,Yummy!', 'Tynnad!,Pretty!', 'Rymmuiñ!,Smile!', 'Shri!,Smile!', 'Thiah!,Sleep!', 'Tharai,Maybe', 'Dei,Correct', 'Iakhun!,Fight!', 'Iashoh!,Fight!', 'Thied!,Buy!', 'Die!,Sell!', 'Rwai!,Sing!', 'Lehkai!,Play!', 'Tem!,Play!', 'Khynñiat,Push', 'Dwai,Pray', 'Hynrei balei?,But why?', 'Sa kaei?,What else?', 'Naduh mynno?,Since when?', 'Mynno kata?,When was that?', 'Ka jingkhyllah!,How weird is that?', 'Sngewbha jarjar,Please keep your voice down', 'Baleh aiu?,Why?', 'Ka pang,That hurts', 'Jied iwei,Choose one', 'Jied uwei,Choose one', 'Jied kawei,Choose one', 'Kwah iwei?,Want one?', 'Kwah uwei?,Want one?', 'Kwah kawei?,Want one?', 'Long jaijai,Be cool', 'Ai sha,Give tea', 'Wanrah sha,Bring tea', 'Peit ialade,Watch yourself', 'Da peit!,Watch out!', 'Leh bha!,Watch out!', 'Pyneh ialade!,Pull yourself together!', 'Masi bakhuid!,Holy cow!', 'Mih mynta,Leave now', 'Ieh shadien,Leave it behind', 'Kiei kito?,What are those?', 'Shong khop,Sit tight', 'Shong beit,Sit tight', 'Sngewbha shong,Please sit', 'Khublei shibun!,Thank you very much!', 'Ka slap,It is raining', \"Ngam tip,I don't know\", 'Jisu khrist!,Jesus Christ!', 'Khublei Jisu,Thank you Jesus', 'Haoid kein,Of course', 'Phi kyllut?,Are you deaf?', 'Pha kyllut?,Are you deaf?', 'Me kyllut?,Are you deaf?', 'Sa shisien,One more time', \"Mano thngan?,Who's hungry?\", 'Lah bieij?,Are you crazy?', 'Nyllong me!,My ass!', 'Nga dei,I am', 'Mei mariang,Mother Earth', 'Mei ramew,Mother Earth', 'Ka jingstad!,How smart!', 'Ka jingbieij,How foolish!', 'Ka jingiskuin,How cute!', 'Ka jingsngewtynnad!,How wonderful!', \"Wat khuslai,Don't worry\", 'Bam lut,Eat everything', 'Bam baroh,Eat everything', 'Bam barohkhoit,Eat everything', 'Nga bam,I eat', 'Bam shaw,Eat noodles', 'Bam soh!,Eat fruits!', 'Bam suki,Eat slowly', 'Ieng joit!,Stand up!', 'Shuh shatai,Stand aside', 'Wat wit,Stand aside', 'Shuh nangne,Stand aside', 'Ieng beit!,Stand still!', 'Ieng khop!,Stand still!', \"Ka shit,It's hot\", \"Ka khluit,It's hot\", 'Ho oah,Yes', 'Hm mm,Yes', 'Mm huh,No', 'Iarap ianga,Help me', \"Ngam sngewthuh,I don't understand\", 'Lah thikna?,Are you sure?', \"Map ianga,I'm sorry\", 'Kloi kloi,Hurry up', 'Wut wut,Hurry up', \"Ngin rem,We'll fail\", 'Iohsngew ianga?,Can you hear me?', 'Kren jem,Speak softly', 'Kren suki,Speak softly', 'Wan kloi!,Come quick!', 'Khublei baroh,Thanks everyone', \"Ka pyngngad,It's refreshing\", 'Kham eh,Louder', 'Kham jam,Louder', 'Kham jarjar,Quieter!', 'Lah iap?,Dead?', 'Bymlah ngeit,Unbelievable!', 'Khatduh-khatwai!,Finally!', 'Shiteng khlieh,Crazy!', 'Ai chalk,Give me a piece of chalk', 'Lah dep,Done', 'Khreh kot!,Study!', 'Lah biang!,Enough!', 'Dep shato!,Whatever!', 'O te,Duh', 'Da kynmaw!,Remember!', 'Thiah suk,Goodnight!', 'Kloi wut!,Quick!', 'Dang shen,Recently', 'Shibun eh!,Very!', 'Lah pei?,Understood?', 'Haba kumta?,Therefore?', 'Balei phi mynjur?,Why did you agree?', 'Balei phi slem?,Why are you late?', 'Hangno phi don?,Where are you?', 'Haei phi don?,Where are you?', \"Lano ka bammiet?,When's dinner?\", 'Lano sngikha jongphi?,When is your birthday?', 'Lano kut skul?,When is school over?', 'Lano kan sdang?,When does it begin?', 'Nga don thyllieh,I have dandruff', 'Shaei ka jhat?,Where is the ship?', \"Nga dang duhnong,I'm losing business\", 'Ka pang kato,That hurts', \"Ka pynshrai bha,It's really annoying\", 'Sngewbha try iwei,Please try one', 'Sngewbha try uwei,Please try one', 'Sngewbha try kawei,Please try one', 'Sngewbha pyrshang uwei,Please try one', 'Sngewbha pyrshang kawei,Please try one', 'Sngewbha pyrshang iwei,Please try one', 'Nga pang jingshong,My butt hurts', 'Tap ki khmat,Cover your eyes', 'Sngewbha tap ialade,Please cover yourself', 'Phi dih sha,You drink tea', 'Pha dih sha,You drink tea', 'Me dih sha,You drink tea', 'Da peit bha!,Watch out!', 'Tan kan plie,Pull it open', 'Bapli ka miaw,Poor cat', 'Ka miaw bapli,Poor cat', 'Bapli u miaw,Poor cat', 'U miaw bapli,Poor cat', 'Bapli i miaw,Poor cat', 'I miaw bapli,Poor cat', 'Shano phi don?,Where are you?', 'Shaei phi don?,Where are you?', 'Iaishah bad nga,Bear with me', 'Iashah bad nga,Bear with me', 'Ngi kren nongjapan,We speak Japanese', 'Tom hadien me!,Tom behind you!', 'Tom hadien phi!,Tom behind you!', 'Tom shadien me!,Tom behind you!', 'Tom shadien phi!,Tom behind you!', 'Tom nadien phi!,Tom behind you!', 'Tom nadien me!,Tom behind you!', 'Shaei ka painkhana?,Where is the toilet?', 'Haei ka painkhana?,Where is the toilet?', 'Shano ka painkhana?,Where is the toilet?', 'Hangno ka painkhana?,Where is the toilet?', 'Kito kidei kiei?,What are those?', 'Katno baje mynta?,What time is it now?', \"Ym lei lei,You're welcome\", 'Nga ieid iaphi,I love you', 'Sa iapeit lashai,See you tomorrow', 'Em kam slap,It is not raining', 'Ngam don bor,I have no energy', 'Nga donkam iaphi,I need you', 'Nga donkam kane,I need this', 'Bha bha khublei,Very good thank you', 'Nga kyrteng Tom,My name is Tom', 'Phi kyrteng aiu?,What is your name?', 'Phi leh aiu?,What are you doing?', \"Ngam don por,I don't have time\", \"Nga shu ongkai,I'm just joking\", 'Khlieng ki tiar,Wash the dishes', 'Sait ki tiar,Wash the dishes', 'Khlieng ki pliang,Wash the dishes', 'Sait ki pliang,Wash the dishes', 'Thet ki kti,Wash your hands', 'Sait ki kti,Wash your hands', 'Bta ka khmat,Wash your face', 'Sait ka khmat,Wash your face', 'Khulom jongno kine?,Whose are these pens?', 'Namar ka sngewshitom,Because she felt sick', 'Biang bha khublei,Very good thank you', 'Phi kyllut ne?,Are you deaf?', 'Pha kyllut ne?,Are you deaf?', 'Me kyllut ne?,Are you deaf?', 'Sa shisien seh,One more time please', 'Sa shisien sngewbha,One more time please', \"Don sa kawei,There's one more\", \"Don sa iwei,There's one more\", \"Don sa uwei,There's one more\", 'Ngan sa leh,I will do it', 'Phi kyrteng kumno?,What is your name?', 'Kumno phi kyrteng?,What is your name?', 'Kyrteng kumno phi?,What is your name?', 'Kyrteng kumno maphi?,What is your name?', 'Kyrteng aiu phi?,What is your name?', 'Kyrteng aiu maphi?,What is your name?', 'Phi iohthiah bha?,Did you have a good sleep?', \"Ngam tip balei,I didn't know why\", \"Mano ba thngan?,Who's hungry?\", 'Ka bha kane,This is good', 'Ka best kane,This is good', 'U kynjat iaka,He kicked it', \"Kan nym her,It won't fly!\", \"In nym her,It won't fly!\", 'Doh jingshong jongnga,Kiss my ass', 'Nga shu pynthikna,I was just making sure', \"Nga khlem tam,I didn't pick it up\", \"Nga khlem khar,I didn't pick it up\", 'Nga lah wan,I am coming', \"Nga shu kylli,I'm just asking\", 'Kumno phi long?,How are you?', 'Ieid iaka pyrthei,Love the earth', \"Nga thngan palat!,I'm so hungry!\", 'Nga don jingieng,I have an erection', 'Song ki tiar,Pack your bags', 'Song ki pla,Pack your bags', 'Lum ki pla,Pack your bags', 'Lum ki tiar,Pack your bags', \"Ka shit mynta,It's hot today\", \"Ka shit palat,It's too hot\", \"Ka khluit palat,It's too hot\", 'Ka kheit syntiew,She picked flowers', 'Ka phut syntiew,She picked flowers', 'Ka tam syntiew,She picked flowers', 'Ka khar syntiew,She picked flowers', 'Nga ieid iapha,I love you', 'Nga ieid iame,I love you', 'Phi kren english?,Do you speak English?', 'Phi kren phareng?,Do you speak English?', 'Haei phi sah?,Where do you live?', 'Phi dei mano?,Who are you?', 'Phi dei iei?,Who are you?', 'Phi kwah aiu?,What do you want?', 'Aiu phi kwah?,What do you want?', 'Kaei phi kwah?,What do you want?', \"Ngin sa rem,We'll fail\", \"Phi ong aiu?,What're you saying?\", \"Phi kren aiu?,What're you saying?\", 'Nga bishni iaphi,I was jealous of you', 'Ki iakylliang jingaikhublei,They exchanged hellos', 'Bud iaki khlur,Follow the stars', 'Sangeh Ka smieij,Stop That tickles', 'Sangeh pynsmieij ianga,Stop tickling me!', \"Wat pynsmieij ianga,Don't tickle me!\", \"Wat ktik ianga,Don't tickle me!\", 'Sangeh ktik ianga,Stop tickling me!', 'Ka eh kane,This is difficult', 'Kane ka eh,This is difficult', 'Kane ka jynjar,This is difficult', 'Ka jynjar kane,This is difficult', 'Shu khie joit,Just get up', 'Pule shuwa iakane,Read this first', 'Pule iakane shuwa,Read this first', 'Iakane shuwa pule,Read this first', 'Ngan tip kumno?,How should I know?', 'Ka peit ianga,She looked at me', 'I peit ianga,She looked at me', 'Nga khublei iaphi,I thank you', \"Wat bakla biang,Don't repeat the same mistake!\", 'Ka khriat bha,I was so cold', \"Nga ioh beer,I've got beer\", \"Ngan don hangne,I'll be right in here\", \"Ngin iarap iaka,We're going to help her\", \"Ki ialeh aiu?,What're they doing?\", \"Aiu ki ialeh?,What're they doing?\", 'Ka pynlyngngoh ianga,It took me by surprise', 'U donkam pisa,He is in need of money', 'Ka sngewieid bha,That was really sweet', \"Shaei u director?,Where's the director?\", \"Shaei ka director?,Where's the director?\", \"Shaei i director?,Where's the director?\", 'U sngapjar shiphang,He remained silent for a while', 'Long ngan call?,Can I make a phone call?', \"U khapniah jingmut,He's closed-minded\", 'Ki sah shapoh,They live downstairs', \"Ka dang khapñiah,It's still crowded\", \"Ngam jied bam,I'm not usually picky about food\", \"Nga bam naphang,I'm not usually picky about food\", 'Ka shait sngewtynnad,It used to be such fun', 'Mat ia dep!,Whatever!', 'Lah leit mo!,Goodbye!', 'Sa leit ho!,Goodbye!', 'Da kumno kumno,Anyway', 'Man ka por,Always', 'Ha ka jingshisha,Honestly', 'Don ba wan?,Did anybody come?', 'Bat ia u,Grab him', 'Don ba iohsngew ianga?,Does anybody hear me?', 'Sa kaei don hangto?,What else is there?', 'Katno baje wai skul?,When is school over?', 'U ieng hangto shiphang,He stood there for a while', 'Ka bneng ka shai,The sky brightened', 'Nga don u paralok,I have a friend', 'Nga don ka paralok,I have a friend', 'Nga don i paralok,I have a friend', 'Jingshong jongnga ka pang,My butt hurts', \"Ngan kynjat jingshong jongphi,I'll kick your butt!\", 'Phi ia dih sha,You drink tea', 'Me ia dih sha,You drink tea', 'Pha ia dih sha,You drink tea', 'Pule kane ka kot,Read this book', 'Phi thied eiei ianga?,Did you buy me anything?', 'Ki dei ki nongjapan?,Are they Japanese?', 'Peit shadien jong phi,Look behind you', 'Peit shadien jong me,Look behind you', 'Peit shadien jong pha,Look behind you', 'Peit hadien jong phi,Look behind you', 'Peit hadien jong me,Look behind you', 'Peit hadien jong pha,Look behind you', 'Tom hadien jong me!,Tom behind you!', 'Tom hadien jong phi!,Tom behind you!', 'Tom shadien jong me!,Tom behind you!', 'Tom shadien jong phi!,Tom behind you!', 'Tom nadien jong phi!,Tom behind you!', 'Tom nadien jong me!,Tom behind you!', 'U don phra snem,He is eight', 'U mih phra baje,He leaves at eight', 'Kiei kito ki nombar?,What are those numbers?', 'Khot kumno ia kito?,What are those called?', 'Khot aiu ia kito?,What are those called?', \"Ngam pat don khun,I don't have any children yet\", 'Haei phi don mynta?,Where are you now?', 'Phi don por lashai?,Do you have time tomorrow?', 'Ngi leit sha iew,We are going to the market', 'Nga donkam ia kane,I need this', 'Nga nang ban jngi,I know how to swim', 'Ka donkam ban sait,It needs washing', \"Ngam nang ban jngi,I don't know how to swim\", 'U Jisu u iam,Jesus wept', 'Kaei ka kyrteng jongphi?,What is your name?', 'Nga dang bam ja,I am eating rice', 'Kine ki khulom jongno?,Whose are these pens?', 'Jongno kine ki khulom?,Whose are these pens?', 'Namar ka don hangto,Because it is there', 'Da peit ka lynti,Watch the road', 'Da peit ka surok,Watch the road', 'Nga dang shu khie,I just got up', 'Ka pyndom ia u,She teased him', 'Ka pynshrai ia u,She teased him', \"Nga khlem tip balei,I didn't know why\", \"Mano ba lah thngan?,Who's hungry?\", 'Ka bha bha kane,This is very good', 'Ka best bha kane,This is very good', 'Phi sngewtynnad ka film?,Do you like the movie?', \"Phin nym doh ianga?,Aren't you going to give me a kiss?\", 'Phi lah bieij ne?,Are you crazy?', 'Me lah bieij ne?,Are you crazy?', 'Pha lah bieij ne?,Are you crazy?', 'Nga dang pule Japanese,I am learning Japanese', 'Ka wah ka tuid,A river flows', 'Peit ha ka dur,Look at the picture', 'Ngi sah ha pyrthei,We live on Earth', \"Nga lah thngan palat,I'm so hungry!\", 'Ka siat iaka helicopter,She shot down a helicopter', 'U ban iaka khyllung,He is lying on the baby', \"Nga biang ia mynta,I'm OK for now\", \"Ka dang ring mynsiem,It's still breathing\", \"Ngin sa ia rem,We'll fail\", \"Phim tip iaka system,You don't know the system\", 'Ki khlur ki itynnad,Stars are beautiful', 'Ki khlur ki mih,The stars came out', 'U bnai u shai,The moon is shining', 'U bnai u phyrnai,The moon is shining', 'Ka khylliap iaka nep,She folded the quilt', 'Kumno ngin leh iakata?,How are we going to do that?', 'Ngin leh kumno iakata?,How are we going to do that?', 'Ka ieid ban shet,She loves to cook', 'Nga kwah iaphi Tom,I want you Tom', 'Nga kwah iame Tom,I want you Tom', 'Kumno ngan shu tip?,How should I know?', 'Ngan shu tip kumno?,How should I know?', \"Nga dei ka kristan,I'm a Christian\", 'Ki la khot iaphi,They called you', 'Nga kynmaw ia kata,I remember that', 'Nga kynmaw ia kato,I remember that', 'Nga kynmaw ia katei,I remember that', 'Nga kynmaw ia katai,I remember that', 'Kum kynrad kum nongbud,Like master like disciple', \"Wat thok Iathuh hok,Don't tell a lie Be honest\", \"Ngi la wan iing,We've come home\", 'U lehkai beach volleyball,He plays beach volleyball', 'Kata kalong kaba sniewbok,That was unfortunate', 'Sngewbha mih na kamrashet,Please get out of the kitchen', 'Nga hap ai jingkren?,Do I have to make a speech?', 'Shaei de ngan leit?,Where else would I go?', 'Phet na iing jongnga,Get out of my house!', 'Phi dei ban sngapjar,You must keep quiet', \"Ngan shim ka jacket,I'll take the jacket\", 'Nga kwah ban iashoh,I want to fight', \"Balei ngam sngewthuh phareng?,Why don't I understand English?\", \"Balei ngam sngewthuh English?,Why don't I understand English?\", \"Nga isih beit iaka,I've always hated her\", 'Nga sngew sieh iaki,I feel bad for them', 'Phi nang ktien aiu?,What languages do you know?', 'U jrong bad khlaiñ,He is tall and strong', \"Phi khlem pynpait iaka,You didn't break it\", \"Phim shym pynpait iaka,You didn't break it\", \"Ngam pyrkhat kum maphi,I don't think like you\", \"Ngam pyrkhat kum mame,I don't think like you\", \"Ngam pyrkhat kum mapha,I don't think like you\", 'Nga khlem iohsngew iaphi,I did not hear you', 'Ngam shym iohsngew iaphi,I did not hear you', \"Phim pat pyrshang hi,You haven't even tried\", 'Jia aiu ia nga?,What is wrong with me?', 'Ka pynkyndit ia nga,It took me by surprise', \"Sa iakynduh ha ïng,I'll see you at home\", \"Nga kwah jam sohplom,I'd like some plum jam\", \"Ngi don aiu-aiu,We've got something\", \"Balei kim don hangne?,Why aren't they here?\", \"Map Nga wan slem,I'm sorry I came late\", 'Baroh lah shah ktah,Everyone has been affected', \"Nga khlem vote iaphi,I didn't vote for you\", 'Don tennis court hangne?,Is there a tennis court around here?', \"Balei phim sngewtynnad ianga?,Why don't you like me?\", 'Ngin pyrshang ban iarap?,Should we try to help?', 'Une udei u jongnga,That is mine', 'U Tom u demkhohsiew,Tom was kneeling', 'Ki juti jongphi kine?,Are these your shoes?', 'Ki juti jongme kine?,Are these your shoes?', 'Ki juti jongpha kine?,Are these your shoes?', 'Mat ia dep shato!,Whatever!', 'Balei phi ia lyngngoh?,Why are you all shocked?', \"Kaei ka ban kylla?,What'll change?\", \"Nga don kum ito,I've got one just like that\", \"Nga don kum kato,I've got one just like that\", \"Nga don kum uto,I've got one just like that\", \"Ka khlem iaid bha,It didn't go well\", \"Ka khlem long bha,It didn't go well\", 'Ka dukan ka khang lashai,The store will be closed tomorrow', 'Shim da kawei ka shuki,Take the other chair!', 'Kane ka shuki ka isih,This chair is ugly', 'Kine ki shuki ki pher,These chairs are different', 'Kane ka shuki ka sting,This chair is light', 'ka bneng ka i dom,The sky looks angry', 'Phi iohi ia ka kper?,Do you see the garden?', 'Phi dei ban pyndonkam deo,You should use a deodorant', 'Ka jaka ka sboh bha,The land is very fertile', 'Ka khyndew ka sboh bha,The land is very fertile', 'Ka phong ka coat shñiuh,She was wearing a fur coat', 'Nga dang wad dukan khapsñiuh,I was looking for the barbershop', 'Ki masi ki ai dud,Cows give milk', 'Ki masi ki bam phlang,Cows eat grass', 'ki masi ki don reng,Cows have horns', 'U dngiem u dait ialade,The bear bites itself', 'Hangne sah uwei u dngiem,Here lives one bear', 'U lah don phra snem,He is eight', 'U mih por phra baje,He leaves at eight', 'Ka painkhana ka don shajrong,The toilet is upstairs', 'Ka painkhana ka don hajrong,The toilet is upstairs', 'Ka painkhana ka don shalor,The toilet is upstairs', 'Ka painkhana ka don halor,The toilet is upstairs', 'Kito kidei ki nombar aiu?,What are those numbers?', 'La khot kumno ia kito?,What are those called?', 'La khot aiu ia kito?,What are those called?', 'Hooid nga don por lashai,Yes I have time tomorrow', 'Ki khynnah ki donkam jingieid,Children need loving', 'U Jisu u jubab iaki,Jesus answered them', 'U Jisu u isih iaphi,Jesus hates you', 'U Jisu u ieid iaphi,Jesus loves you', 'Kane ka dei ka miaw,This is a cat', 'Kine ki dei ki khulom,These are pens', 'Ai sha nga u khulom,Pass me the pen', 'Ai ha nga u khulom,Pass me the pen', 'U Tom u i hok,Tom seems sincere', \"Ngam kwah ban pynthut iaphi,I don't want to disturb you\", 'Ban ksem ka long khlemakor,Farting is rude', \"Phim lah ban lait nanga,You can't escape from me\", \"Phim lah ban phet nanga,You can't escape from me\", \"Mem lah ban lait nanga,You can't escape from me\", \"Pham lah ban lait nanga,You can't escape from me\", 'Ym donkam ban leh iakato,There is no necessity for you to do that', 'Ym donkam ban leh iakata,There is no necessity for you to do that', 'Nga iohi ia u lum,I saw the hill', 'U lum u jyrngam beit,The hill is always green', 'Ngi iaid shajrong u lum,I walked up the hill', 'Ka wah ka lah shlei,The river overflowed', 'Ngim ju kren phareng hangne,English is not spoken here', 'Ka pyrthei ka shad pyllun,The earth rotates', 'Nga ieid ia u Blei,I love God', \"Ka shit mynta ka sngi,It's hot today\", 'U ban ia i khyllung,He is lying on the baby', 'U ban ia u khyllung,He is lying on the baby', \"Nga thikna phi bunkam bha,I'm sure you're very busy\", \"Nga thikna pha bunkam bha,I'm sure you're very busy\", \"Nga thikna me bunkam bha,I'm sure you're very busy\", 'Don ki dewlynnong haka duriaw,There are islands in the sea', 'Nga don artylli ki miaw,I have two cats', \"Ngam kwah ban leit skul,I don't want to go to school\", 'Phi lah don katno snem?,How old are you?', 'Nga lah kyrshan iaphi shilynter,I have supported you throughout', \"Nga don hangne ban sah,I'm here to stay\", 'Phi lah ban iohsngew ianga?,Can you hear me?', 'U bnai u lah sep,The moon has set', 'Kane ka long kaba eh,This is difficult', 'Kane ka long kaba jynjar,This is difficult', 'Hynrei kidei lut ki briew,But they are all people', \"Nga khlem mut ban peitseh,I didn't mean to stare\", \"Nga khlem kwah ban leh,I didn't want to do it\", \"Ngan nym leit phai biang,I'm not going to go back\", 'Me ioh ban peit iaka,Did you get a good look at her?', 'Ngam donkam shuh iaka loan,I no longer need a loan', 'U Tom u i sngewsih,Tom looks sad', \"Ka long ka bymlahlong ianga,That's impossible for me\", \"Nga lah isih beit iaka,I've always hated her\", 'Phin ia sngewbha sangeh iashoh,Will you guys please stop fighting?', 'U shong haka seat bashakhmat,He is sitting in the front seat', \"Phi dei uba heh tam,You're the biggest\", \"Phi long uba heh tam,You're the biggest\", 'Nga sngew kum ka rit,I feel like shit', \"Jia aiu mynta ha Poland?,What's happening now in Poland?\", 'Phi don samla ba biang?,Do you have a steady boyfriend?', 'Nga lah pynjrong iaka shuti,I extended my holiday', 'Nga lah pynjlan iaka shuti,I extended my holiday', \"Ngam tharai kin iohsngew iangi,I don't think they're going to hear us\", 'Phi lah poi sha Minnesota?,Have you ever been to Minnesota?', 'Phi lah sait ki kti?,Have you washed your hands yet?', 'U poi hangne dang mynta,He arrived here just now', 'U poi shane dang mynta,He arrived here just now', 'Kyrteng aiu kane ka wah?,What is the name of this river?', 'Ai katba pan u Tom,Give Tom everything he asks for', 'Phim sngewbha leit re shata?,Will you please go there?', \"Nga sngewthuh balei phi dom,I understand why you're angry\", \"Ngi don aiu-re-aiu,We've got something\", \"Ngi don kaei-re-kaei,We've got something\", \"Ngi don uei-re-uei,We've got something\", 'Ki juti nga ki brown,My shoes are brown', 'U Tom u kwah iaphi,Tom wants you', 'U Tom u kwah jubab,Tom wants answers', 'U kynthih ia ka thliew,He jumped over a ditch', \"Phim lah ban thied iakane,You can't afford this\", \"Ngan nym iathuh iaki kata,I'm not going to tell them that\", 'Ka jylliew kane ka nan?,Is this lake deep?', \"Nga khlem dih ka um,I didn't drink the water\", 'Phi poi iohi iaka mynnin?,Did you happen to see her yesterday?', \"Shu iathuh ianga jia aiu,Just tell me what's going on\", 'Kane ka dei ka jongnga,This belongs to me', \"Ngi kwah kaba bha iaphi,We want what's best for you\", \"Ngan leit sha Australia lashembnai,I'm going to Australia the month after next\", 'Khublei na ka bynta baroh,Thanks for everything', 'U Tom u dang demkhohsiew,Tom was kneeling', 'U lah pynhiar kyrdan iaphi,He demoted you', 'U lah pynhiar kyrdan iame,He demoted you', 'U lah pynhiar kyrdan iapha,He demoted you', 'Shano phi kwah ban leit?,Where would you like to go?', 'Shano me lwah ban leit?,Where would you like to go?', 'Shano pha kwah ban leit?,Where would you like to go?', 'Shaei phi kwah ban leit?,Where would you like to go?', 'Shaei pha kwah ban leit?,Where would you like to go?', 'Shaei me kwah ban leit?,Where would you like to go?', 'Kine kidei ki juti jongphi?,Are these your shoes?', 'Kine kidei ki juti jongme?,Are these your shoes?', 'Kine kidei ki juti jongpha?,Are these your shoes?', 'Don mano mano ba wan?,Did anybody come?', 'Ki leh kumba ki tieng,They pretended they were afraid', 'Phi leh aiu katto baje?,What were you doing at that time?', 'Me leh aiu katto baje?,What were you doing at that time?', 'Pha leh aiu katto baje?,What were you doing at that time?', \"Ngam ju dei kaei-kaei ruh,I can't get anything right\", \"Nga sngewbha shibun ban don hangne,I'm very pleased to be here\", 'Phi don eiei ban kylli ianga?,Is there something you want to ask me for?', 'Nga ieid ia katai ka shuki,I love that chair', \"Ngam khot iaphi da ka kyrteng,I'm not calling you by name\", \"Ka long ka bymlahlong ban shna,It's impossible to fix\", 'U Tom u shoh shwa ianga,Tom hit me first', 'Ka ksieh ia kawei ka miaw,She strangled a cat', \"Phi mut ba nga leh bakla?,Do you think I'm making a mistake?\", \"Me mut ba nga leh bakla?,Do you think I'm making a mistake?\", \"Pha mut ba nga leh bakla?,Do you think I'm making a mistake?\", 'Ka thoh ianga ka shithi bajrong,She wrote me a long letter', 'Baroh ki han-blei ki lieh,All swans are white', 'Ki han-blei ki lieh lut,All swans are white', 'Ki lieh lut ki han-blei,All swans are white', 'Ki masi ki ai dud iangi,Cows give us milk', 'U don shiphew tylli ki masi,He has ten cows', 'Ka jingshisha ka pyni iaka jingisat,The truth bears hatred', 'Ka jingdom ka dei ka bor,Anger is an energy', 'U kofi u ai bor iaphi,Coffee gives you energy!', 'Ka khriat bha mynta ka sngi,It is very cold today', 'Kine ki khulom kidei jong u,These pens are his', 'Kumno ka long ka jingpule jongme?,How are your studies going?', 'Balei ia kane hap ban pyndep?,Why does this have to be done?', \"Phim dei tang maphi ba thngan?,You're not the only one who's hungry\", \"Mem dei tang mame ba thngan,You're not the only one who's hungry\", \"Ngam dei tang manga ba thngan,I'm not the only one who's hungry\", 'Kumno la pyndonkam iakane ka kamera,How do you use this camera?', 'Ka ksem jong u ka sma,His fart smelled', 'Ban ksem ka long kaba khlemakor,Farting is rude', \"Phin ym lah ban lait nanga,You can't escape from me\", \"Phin ym lah ban phet nanga,You can't escape from me\", \"Phin nym lah ban lait nanga,You can't escape from me\", \"Phin nym lah ban phet nanga,You can't escape from me\", \"Men ym lah ban lait nanga,You can't escape from me\", \"Men nym lah ban lait nanga,You can't escape from me\", \"Men ym lah ban phet nanga,You can't escape from me\", \"Men nym lah ban phet nanga,You can't escape from me\", \"Phan ym lah ban lait nanga,You can't escape from me\", \"Phan nym lah ban lait nanga,You can't escape from me\", 'U Tom u kwah aiu tip,I wonder what Tom wants', \"Phim donkam ban leit skul lashai,You don't have to go to school tomorrow\", 'Nga kwah klet noh shaphang kato,I just want to forget about it', \"Ngam kwah ban leit sha skul,I don't want to go to school\", 'Ka khot sngewbha ianga ban bam,She invited me to eat', 'Don tang ki kot ha duli,There are only books on the shelf', 'Tom bad Mary ki don shabar,Tom and Mary are outside', 'Ki khlur ki long kiba itynnad,Stars are beautiful', 'Nga kwah iaphi ban doh ianga,I want you to kiss me', 'Ngi lah dei ban iapoi biang,We should be getting back', \"Baroh ki don eiei ban buhrieh,Everybody's got something to hide\", \"Ngam kwah tip eiei shaphang kata,I don't want to know anything about that\", 'Kiei ki ktien ba phi nang?,What languages do you know?', 'Nga iakynduh ia u ha jingkhawai,I met him at a party', \"Ka lah slem ban leh iakata,It's too late to try that\", \"Ka lah dier ban leh iakata,It's too late to try that\", 'U mih khlem khang ka jingkhang,He left without having shut the door', 'Phi lah ju poi sha Minnesota?,Have you ever been to Minnesota?', 'Balei phi donkam TV ba thymmai?,Why do you need a new television?', 'U dih sha man ka step,He has tea every morning', 'U don ha ka jingdonkam pisa,He is in need of money', 'Kane ka wah ka kyrteng aiu?,What is the name of this river?', 'Aiu ka kyrteng kane ka wah?,What is the name of this river?', 'U Tom u hap naki mawstep,Tom fell down the stairs', \"Ngam pat iohi iaka lah slem,I haven't seen her for a long time\", 'Kumno ngan pyniapbieij ia u Tom?,How am I going to impress Tom?', 'Jied ia iba phi sngewtynnad tam,Pick the one you like best', 'Jied ia iba phi best tam,Pick the one you like best', 'U Tom un pynmynsaw ia phi,Tom is going to hurt you', 'U Tom un pynmynsaw ia pha,Tom is going to hurt you', 'U Tom un pynmong ia me,Tom is going to hurt you', 'U Tom un pynmong ia phi,Tom is going to hurt you', 'U Tom un pynmynsaw ia me,Tom is going to hurt you', 'U Tom un pynmong ia pha,Tom is going to hurt you', \"Ngan sa iakynduh iaphi ha ïng,I'll see you at home\", \"Ngan sa iakynduh iame ha ïng,I'll see you at home\", 'One I dei I tiket kot,This is a paper ticket', 'Ngi trei na bynta u Tom,We work for Tom', 'U Tom u sdang ban khih,Tom began to move', 'Phi tip katno ngi lah iohnong?,Do you know how much profit we made?', 'Nga lah leh katba nga leh,I gave it my best shot', \"Ngan leit sha office u Tom,I'm going to Tom's office\", 'U Tom u jop iaka election,Tom won the election', 'Ngi bam lang man ka janmiet,We eat together every evening', 'Naduh mynno phi lah pule Latin?,Since when have you been learning Latin?', 'Ki kwah biang ia u Tom,They want Tom back', \"Phim kwah pynsangeh ia u Tom?,Don't you want to stop Tom?\", 'Ka ïng jongnga ka khriat bha,My house is very cold', \"Ka phone u Tom ka pah,Tom's phone rang\", 'Nga tip phi don maphi kato,I know you have it with you', 'U Tom u pynlait im ianga,Tom just saved my life', 'Nga sngewtynnad ki briew kiba pynsamrkhie,I like funny guys', 'Maka ruh ka tip ia kata,She knows that as well', \"Ngam bam re kane ka dohkha,I'm not eating this fish\", 'U Sami um tieng iaka khlaw,Sami is not afraid of the jungle', \"Um i kumba u lah thait,He doesn't look like he's tired\", 'Ka ong ba ka la kdang,She said that she was full', 'Nga park ia ka kali hangtei,My car is parked over there', \"Kim pat dep bam ja sngi,They haven't finished their lunch yet\", 'U Tom u dang pynleit jingmut,Tom is still paying attention', 'U Sami um ju burom ianga,Sami was never respectful to me', \"Kine ki patlun kim biang ianga,These pants don't fit me\", \"Kine ki patlun kim ñiam ianga,These pants don't fit me\", \"Kane ka patlun kam biang ianga,These pants don't fit me\", \"Kane ka patlun kam ñiam ianga,These pants don't fit me\", 'U Sami u la shah kem,Sami was arrested', 'U don dakmong ha kjat kamon,He has a bruise on his right leg', 'Ngam tip bha shaphang jong u,I knew very little about him', 'U dei hok ba un sngapjar,He was right to keep silent', 'Long phin khot ia u Tom?,Would you mind calling Tom for me?', 'Ngi dei ban pyrshang ia kane,We should give this a try', \"Ka don bad ki ksew jongka,She's with her dogs\", 'Kane kadei na ka bynta jongnga,This is for me', 'Une udei na ka bynta jongnga,This is for me', \"Yn ym don da kawei ka por,There won't be a next time\", 'Pynpait ki pylleng bad weng ki shangai,Break the eggs and remove the yolks', 'Ka nar ka ran ynda lah pjah,Metal contracts when cooled', 'U khublei ianga da ki khmat jongu,He thanked me with his eyes', 'Phi tip dei shaphang kaei kine baroh?,Do you know what this is all about?', \"Ka khlem da long kumba nga mut,It didn't work out quite like I intended it to\", 'Phi ia mynjur bad u Tom dei?,You agree with Tom right?', 'U Tom u dang shu dep pynkhreh,Tom is just finishing setting up', 'Ki masi ki im da u phlang,Cows live on grass', 'Ki masi ki ai iangi ka dud,Cows give us milk', 'Ka jingshisha ka pynmih ia ka jingisih,The truth bears hatred', 'Ka jingshisha ka pynmih ia ka jingiashun,The truth bears hatred', 'Ki dngiem ki lah ban kiew dieng,Bears can climb trees', 'Balei ba nga hap ban nang french,Why do I need to learn French?', 'Ka Emily ka isih ia ki painkhana,Emily hates toilets', 'Nga leid ia ktien japan bad korea,I like Chinese and Japanese', 'Kine ki dei ki khulom jong u,These pens are his', 'U practice tem guitar baroh shi miet,He practices playing the guitar far into the night', 'Phi nang ban pyndonkam ia ki chopsticks?,Do you know how to use chopsticks?', 'U Brian u sah hajrong u lum,Brian lives over the hill', 'Phi lah ban iathuh ia nga lashai,You can tell me tomorrow', 'Nga hap ban ai jop ia ka,I had to let her win', 'Khublei ba phi lah pynshet iaka um,Thank you for boiling the water', 'Nga shim iaka kum ka dak jingkyrmen,I take it as a sign of hope', 'Ka pyrthei ka rkhie da ki syntiew,Earth laughs in flowers', 'U Ned u bad pynieng iaka lama,Ned held the flag erect', 'U Tom u deng ka iitkhmat jongu,Tom put on his sunglasses', 'Phi kwah ban wan shong bad nga?,Do you want to come sit by me?', 'Ngi dei ban mih noh mynta mynta,We must leave immediately', 'Ka jingshisha ka long ba nga thok,The truth is I told a lie', \"Balei ki mrad kim lah ban kren?,Why can't animals talk?\", 'Kine ki mrad ki long paralok bha,These animals are friendly', \"Kam dei ka snam dei u bit,It's not blood It's beet\", \"Balei um ju bamjasngi shuh bad nga?,Why doesn't he eat lunch with me anymore?\", \"Ki ia tieng lut i'u Tom,They're all scared of Tom\", 'Phi lah ban leit lada phi kwah,You can go if you want to', 'Me lah ban leit lada me kwah,You can go if you want to', 'Pha lah ban leit lada pha kwah,You can go if you want to', 'Nga call sha office jong u mynin,I called at his office yesterday', 'Mynta nga thoh da ki dak thymmai,Now I write with the new font', 'Phi lah ban leh katba phi mon,You may do what you wish', 'Iathuh ianga haei ka waiñ ka don,Tell me where the wine is', 'Iathuh ianga haei ka don ka waiñ,Tell me where the wine is', 'Kane ka kot kadei ka kot thymmai,That book is a new book', 'Kane ka kitab kadei ka kitab thymmai,That book is a new book', \"U Tom u khlem leh bakla eiei,Tom didn't do anything wrong\", 'Nga shu donkam katto-katne u aspirin,I just need some aspirin', 'U Tom u don ha ka surgery?,Is Tom in surgery?', 'Lada phi kwah Jinsuk pynkhreh iaka thma,If you want peace prepare for war', 'U Tom u pankylliang pisa na ngi,Tom borrowed some money from us', 'Don duna um bha ha kophi jongphi,There is very little water in your coffee', '\"Me thiah bad ka!\" \"Nga khlem thiah\",\"You slept with her!\" \"I did not\"', 'Bunsap lane jaituh Hynrei ym baroh ar,Either skillful or lazy But not both', 'Kane ka sentence ka dei jong nga,This sentence is mine', 'Ka buh ia ki khun sha jingthiah,She puts the children to bed', \"Nga shong baroh shi miet thoh jingthoh,I've been up all night writing\", \"Nga khlem ai eiei ia u Tom,I didn't give Tom anything\", \"Ngam shem ai eiei ia u Tom,I didn't give Tom anything\", 'Kaei ka kyrteng jong kane ka wah?,What is the name of this river?', 'Ai ia u Tom katba u pan,Give Tom everything he asks for', 'Mano ba stet tam naki lai ngut?,Who runs the fastest of the three?', \"Ngan sa iakynduh ia pha ha ïng,I'll see you at home\", 'Kan sa ai party la shem taiew,She will give a party next week', 'Ki juti nga ki long rong brown,My shoes are brown', 'Sngewbha tangba kato kadei ka racket jongnga,Pardon me but that is my racket', 'Dei manga ba pyniap ia u Kennedy,It was me who killed Kennedy', 'Haduh katno ngi hap ban ap hangne?,How long do we have to wait here?', 'Ka eriong ka pynngam ia ka lieng,The storm sank the boat', 'Nga dang kylli jingkylli ia i pa,I am asking questions to my father', 'Nga tip phi don essay ban thoh,I know you have an essay to write', 'Phi mut ban leh aiu-aiu de?,What else are you planning to do?', 'Kato ka khubor ka lah saphriang lut,That news got around', 'Nga wan namar me thoh aiu re,I came because of something you wrote', 'I pa i ai game computer ianga,A computer game was given to me by my father', 'Nga pyrshang ban install ka browser thymmai,I tried to install a new browser', 'U pyni iangi kumno ban ñiah kulai,He showed us how to ride a horse', 'Uwei-pa-uwei naki u don kali,Each of them has his own car', 'Balei phi shait kylli bun jingkylli bha?,Why do you always ask so many questions?', 'Ka long ka briew kaba bhabriew bha,She is a very beautiful woman', 'Ka thok ban iada iaka longïing jongka,She lied to protect her family', 'Phi sia phi ia pahuh por shibun,You guys have too much time on your hands', \"Ka lok u Sami ka don hangto,Sami's wife was there\", \"Map Nga khlem mut ban mut eiei,I'm sorry I didn't mean to imply anything\", 'U don dak jingmong ha kjat kamon,He has a bruise on his right leg', 'U don ka hok ban iada ialade,He had the right to defend himself', 'Nga iohi u Tom u mareh shathie,I saw Tom running down the street', \"Lah slem u Tom um pat jngi,Tom hasn't been swimming in a long time\", 'Ngan da bna bha shaphang jong u,I knew very little about him', \"Balei phim ju iathuh ianga shaphang kata?,Why didn't you ever tell me that?\", \"Balei pham ju iathuh ianga shaphang kata?,Why didn't you ever tell me that?\", \"Balei mem ju iathuh ianga shaphang kata?,Why didn't you ever tell me that?\", 'U Kren ianga mynba u ïohi ianga,He spoke to me when he saw me', \"Ki lah tip lypa kin leh aiu,They already know what they'll do\", 'U Tom u don bun ki paralok,Tom has lots of friends', 'Phi leh aiu ha kato ka por?,What were you doing at that time?', 'Me leh aiu ha kato ka por?,What were you doing at that time?', 'Pha leh aiu ha kato ka por?,What were you doing at that time?', 'Kumba katno sngi phin don ha Boston?,About how many days will you be in Boston?', 'Kumba katno sngi phan don ha Boston?,About how many days will you be in Boston?', 'Kumba katno sngi men don ha Boston?,About how many days will you be in Boston?', 'Ka miaw ka paw na khrum ka mieij,A cat appeared from under the desk', \"U Tom u lah dei ban sngewsih bha,Tom must've been very disappointed\", 'Kane ka account kam iamynjur bad ki jingshisha,This account does not agree with the facts', 'Phi leh aiu na ka bynta ka team?,What do you do for the team?', 'Kumno phi tip u Tom u don hangto?,How do you know Tom is there?', 'Kumno me tip u Tom u don hangto?,How do you know Tom is there?', 'Kumno pha tip u Tom u don hangto?,How do you know Tom is there?', 'U khynnah u saitlah ban ngeit iau nonghikai,The student refused to obey his teacher', 'U Tom um ju iohi ia ka Mary,Tom never saw Mary', 'Ka Germany ka dei ka ri ba khriat,Germany is a cold country', \"Nga khlem don por tang ban airong ruh,I didn't even have time to paint!\", 'Phi nang kumno ban pyndonkam ia ki chopsticks?,Do you know how to use chopsticks?', \"Ha kawei pat ka por wat pynhap iaka,Next time don't drop it\", \"Ngam kwah ban tip eiei shaphang u Tom,I don't want to know anything about Tom\", 'Nga hap ban ailad ba kan jop maka,I had to let her win', 'Ki briew ki iakren ha kajuh ka por,People are speaking at the same time', \"Iei i Disney character ba best tam maphi?,Who's your favorite Disney character?\", 'Ka Mary ka lah ialam bakla ia phi,Mary misled you', \"Phi dei maphi ba hap siew iaka bill,You're the one who should pay the bill\", 'Kam ju iohi shuh ia ka hymmen jongka,She never saw her sister again', 'U Tom bad Ka Mary ki don shabar,Tom and Mary are outside', \"Ngam sngewtynnad iakane ka kot Ka phyrnai palat,I don't like this paper It's too shiny\", \"Ngam sngewtynnad iakane ka kot Ka shai palat,I don't like this paper It's too shiny\", \"Ngan leh ia kaei kaba dei ban leh,I'll do what must be done\", 'U Tom u kham bunkam ban ia nga,Tom was even busier than I was', \"U Tom u khlem leh eiei ba bakla,Tom didn't do anything wrong\", 'U Tom un sa iap khlem jingiarap jongphi,Tom will die without your help', \"U Tom um tip eiei u leh aiu,Tom has no idea what he's doing\", \"U Tom um tip eiei aiu u leh,Tom has no idea what he's doing\", \"U Tom um tip eiei ba u leh,Tom has no idea what he's doing\", 'Ka Tokyo ka kham heh ban iaka Yokohama,Tokyo is bigger than Yokohama', 'Nga kwah ba un leit hapoh shiteng sngi,I want him gone by noon', \"Ngam lah ngeit ngi dang ia don hangne,I can't believe we're all still here\", \"Ngam thikna mano ba bat ka suitcase nga,I'm not sure who has my suitcases\", 'Ka Mary ka shet kylla ia u Tom,Mary threw Tom under the bus', 'Nga slem te nga duh iaka flight 501,I was late so I missed flight 501', 'Nga shu kwah ia u ba un phet,I just want him to go away', \"Ngam tip u Tom un wan ne em,I don't know if Tom is coming or not\", 'Jingphohsniew jongphi ki kyllain kum ki jingruma jongnga,Your dreams are almost as twisted as my nightmares', 'Ngi dei ban trei katba ngi dang im,We must work as long as we live', 'Katba ngi dang im ngi dei ban trei,We must work as long as we live', \"Nga lah poi sha Boston katto katne sien,I've been to Boston numerous times\", \"Nga lah leit sha Boston katto katne sien,I've been to Boston numerous times\", \"Nga lah pynlut bun ka jingim jongnga hangne,I've spent most of my life here\", 'U ai kajuh ka jubab kum kaba hashuwa,He gave the same answer as before', 'Mynta ki por ym don ba ngeit rngai,Nowadays no one believes in ghosts', 'Nga ai khublei iaphi naduh ka dohnud jongnga,Thank you with all my heart', 'Ki i kumba ki tip lut shaphang jongngi,They seem to know all about us', \"Phi kylli bun jingkylli bha hato kam dei?,You really do ask a lot of questions don't you?\", 'Phi lah thied lypa ka ticket jong phi?,Have you already bought your ticket?', 'Kumno phi ioh jingbit ban leh ia kata?,How did you get permission to do that?', 'U la phla ba u la leh shiliang,He admitted that he was biased', \"Ki lah lah ban pyndep hi da lade,They could've done it by themselves\", \"Lada dei nga ngan leit kai sha Boston,I'd visit Boston if I were you\", \"Kim shah iaphi ba phin leh kumto hangne,They won't permit you to do that here\", 'Ki iathuh iangi ia kaei ba ki tip,They told us what they knew', \"Um shem lah ban leh hi da lade,He couldn't do that by himself\", 'U Sami unhap ban shaniah ia ka Layla,Sami needs to trust Layla', \"Phi tip mano ba dei kmie u Tom?,Do you know who Tom's mother is?\", \"Phi tip i kmie u Tom dei mano?,Do you know who Tom's mother is?\", \"Ka Mary ka ong ba kan ym leh,Mary said that she wouldn't do it\", \"Ka Mary kam sngewtynnad iaka sofa bathymmai jongka,Mary doesn't like her new sofa\", \"Ka Mary ka ong ba kam bunkam lashai,Mary says she's not busy tomorrow\", 'U ong ba u donkam ban leit thiedjingthied,He said he needs to go shopping', 'Ki ong ba ki tieng ban leit shato,They said they were afraid to go there', 'Ki ong ba ki tieng ban leit shatai,They said they were afraid to go there', 'Ki ong ba ki tieng ban leit shata,They said they were afraid to go there', 'Ki ong ba ki tieng ban leit shatei,They said they were afraid to go there', \"Ngam lah shah shuh iaka jingkhkemakor u Tom,I can't put up with Tom's bad manners any longer\", 'Phim dei ban leh kum kato da lade,You should not do that kind of thing by yourself', 'Mem dei ban leh kum kato da lade,You should not do that kind of thing by yourself', 'Pham dei ban leh kum kato da lade,You should not do that kind of thing by yourself', 'Ka Mary ka song kyrkieh ia ka suitcase,Mary hastily packed her suitcase', 'Kumno sa don ki jaid briew kum maphi?,How do people like you get to be people like you?', 'Kumno sa don ki jaid briew kum mame?,How do people like you get to be people like you?', 'Kumno sa don ki jaid briew kum mapha?,How do people like you get to be people like you?', 'U Sami u long u kpa ba bha,Sami is a good dad', 'U Sami u long u kpa ba biang,Sami is a good dad', 'Nga iathuh lut ia i mei i pa,I told my parents everything', 'U kren ianga por ba u ïohi ianga,He spoke to me when he saw me', 'Nga shu rkhie namar baroh ki ia rkhie,I just laughed because everyone else did', \"U Tom um kwah ban leit shato marwei,Tom didn't want to go there alone\", \"U Tom um kwah ban leit shatei marwei,Tom didn't want to go there alone\", \"U Tom um kwah ban leit shatai marwei,Tom didn't want to go there alone\", 'Nga phong bakla da lyndet ia ki gloves,I put my gloves on inside out by mistake', 'Kane ka kot ka suk ianga ban pule,This book is easy for me to read', \"Ka race shispah mitar kan sdang ar baje mynsngi, The Hundred meter race will start at two o'clock\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 975 sentence pairs\n",
      "Trimmed to 0 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2\n",
      "kha 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-88f6333805da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kha'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('kha', 'eng', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
